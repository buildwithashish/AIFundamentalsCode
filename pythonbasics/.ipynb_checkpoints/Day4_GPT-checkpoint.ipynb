{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99e39f77-a511-4eef-adad-6c06b931f60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next predicted word: The\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Simple tokenizer and vocab\n",
    "vocab = {\"The\": 0, \"cat\": 1, \"sat\": 2, \"on\": 3, \"the\": 4, \"mat\": 5}\n",
    "inv_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "def simple_tokenizer(sentence):\n",
    "    return [vocab[word] for word in sentence.split()]\n",
    "\n",
    "# Positional encoding\n",
    "def positional_encoding(seq_len, dim):\n",
    "    pos = np.arange(seq_len)[:, np.newaxis]\n",
    "    i = np.arange(dim)[np.newaxis, :]\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(dim))\n",
    "    angle_rads = pos * angle_rates\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    return torch.tensor(angle_rads, dtype=torch.float32)\n",
    "\n",
    "# Tiny GPT block\n",
    "class TinyGPTBlock(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embed = nn.Parameter(positional_encoding(10, embed_dim), requires_grad=False)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads=1, batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 2 * embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * embed_dim, embed_dim)\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.output = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        x = self.embed(x) + self.pos_embed[:seq_len]\n",
    "        residual = x\n",
    "        x = self.ln1(x)\n",
    "        attn_out, _ = self.attn(x, x, x, need_weights=False)\n",
    "        x = residual + attn_out\n",
    "\n",
    "        residual2 = x\n",
    "        x = self.ln2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = x + residual2\n",
    "\n",
    "        logits = self.output(x)\n",
    "        return logits\n",
    "\n",
    "# Run the model\n",
    "sentence = \"The cat sat on the\"\n",
    "token_ids = simple_tokenizer(sentence)\n",
    "input_tensor = torch.tensor([token_ids])\n",
    "\n",
    "model = TinyGPTBlock(vocab_size=len(vocab), embed_dim=16)\n",
    "logits = model(input_tensor)\n",
    "\n",
    "next_token_logits = logits[0, -1]\n",
    "probs = F.softmax(next_token_logits, dim=-1)\n",
    "predicted_id = torch.argmax(probs).item()\n",
    "predicted_word = inv_vocab[predicted_id]\n",
    "\n",
    "print(\"Next predicted word:\", predicted_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8deec29-e6ca-4bd3-8d88-7c1a04a30bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cat sat on the floor\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "input_text = \"The cat sat on the\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=1)\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db4957d6-bca3-4232-afcc-98e6af5c8ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 21\n",
      "word2idx: {'The': 0, 'barn': 1, 'bed': 2, 'bird': 3, 'cat': 4, 'cow': 5, 'dog': 6, 'fish': 7, 'flew': 8, 'in': 9, 'mat': 10, 'near': 11, 'on': 12, 'over': 13, 'pond': 14, 'sat': 15, 'slept': 16, 'stood': 17, 'swam': 18, 'the': 19, 'tree': 20}\n",
      "Example input (tokens): [0, 4, 15, 12, 19]\n",
      "Target token ID: 10\n",
      "Input words: The cat sat on the\n",
      "Target word: mat\n",
      "Epoch 1/200, Loss: 3.0697\n",
      "Epoch 20/200, Loss: 0.0000\n",
      "Epoch 40/200, Loss: 0.0000\n",
      "Epoch 60/200, Loss: 0.0000\n",
      "Epoch 80/200, Loss: 0.0000\n",
      "Epoch 100/200, Loss: 0.0000\n",
      "Epoch 120/200, Loss: 0.0000\n",
      "Epoch 140/200, Loss: 0.0000\n",
      "Epoch 160/200, Loss: 0.0000\n",
      "Epoch 180/200, Loss: 0.0000\n",
      "Epoch 200/200, Loss: 0.0000\n",
      "Training done\n",
      "Input: The cat sat on the\n",
      "Predicted next word: mat\n",
      "Input: The dog slept on the\n",
      "Predicted next word: bed\n",
      "\n",
      "Input: The bird flew over the\n",
      "Predicted next word: tree\n",
      "\n",
      "Input: The fish swam in the\n",
      "Predicted next word: pond\n",
      "\n",
      "Input: The cow stood near the\n",
      "Predicted next word: barn\n",
      "\n",
      "âœ… Model and tokenizer saved successfully.\n"
     ]
    }
   ],
   "source": [
    "#TinyGPT training setup from scratch\n",
    "sentences = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"The dog slept on the bed\",\n",
    "    \"The bird flew over the tree\",\n",
    "    \"The fish swam in the pond\",\n",
    "    \"The cow stood near the barn\"\n",
    "]\n",
    "\n",
    "# Build vocabulary\n",
    "all_words = set(\" \".join(sentences).split())\n",
    "word2idx = {word: idx for idx, word in enumerate(sorted(all_words))}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "# Show vocab\n",
    "print(\"Vocabulary size:\", len(word2idx))\n",
    "print(\"word2idx:\", word2idx)\n",
    "\n",
    "# Tokenizer\n",
    "def tokenize(sentence):\n",
    "    return [word2idx[word] for word in sentence.split()]\n",
    "\n",
    "# Detokenizer (for predictions)\n",
    "def detokenize(indices):\n",
    "    return \" \".join([idx2word[idx] for idx in indices])\n",
    "\n",
    "    # Prepare input-output pairs\n",
    "X_data = []\n",
    "y_data = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = sentence.split()\n",
    "    input_words = words[:-1]\n",
    "    target_word = words[-1]\n",
    "\n",
    "    X_data.append(tokenize(\" \".join(input_words)))\n",
    "    y_data.append(word2idx[target_word])\n",
    "\n",
    "# Show one example\n",
    "print(\"Example input (tokens):\", X_data[0])\n",
    "print(\"Target token ID:\", y_data[0])\n",
    "print(\"Input words:\", detokenize(X_data[0]))\n",
    "print(\"Target word:\", idx2word[y_data[0]])\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Add a PAD token to vocab\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "pad_idx = len(word2idx)\n",
    "word2idx[PAD_TOKEN] = pad_idx\n",
    "idx2word[pad_idx] = PAD_TOKEN\n",
    "\n",
    "# Update tokenized X_data to torch tensors\n",
    "X_data_padded = pad_sequence(\n",
    "    [torch.tensor(x, dtype=torch.long) for x in X_data],\n",
    "    batch_first=True,\n",
    "    padding_value=pad_idx\n",
    ")\n",
    "\n",
    "y_data_tensor = torch.tensor(y_data, dtype=torch.long)\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class TinyGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.pos_embedding = nn.Embedding(20, embed_dim)  # assume max 20 tokens\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads=1, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 2 * embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * embed_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.shape\n",
    "        positions = torch.arange(seq_len).unsqueeze(0).expand(batch_size, seq_len).to(x.device)\n",
    "\n",
    "        x = self.embedding(x) + self.pos_embedding(positions)\n",
    "\n",
    "        residual = x\n",
    "        x = self.ln1(x)\n",
    "        x, _ = self.attn(x, x, x, need_weights=False)\n",
    "        x = x + residual\n",
    "\n",
    "        residual2 = x\n",
    "        x = self.ln2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = x + residual2\n",
    "\n",
    "        # Return logits for final token only\n",
    "        return self.output_layer(x[:, -1])\n",
    "\n",
    "\n",
    "# Parameters\n",
    "vocab_size = len(word2idx)\n",
    "embed_dim = 32\n",
    "epochs = 200\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Model, loss, optimizer\n",
    "model = TinyGPT(vocab_size, embed_dim, pad_idx)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(X_data_padded)            # shape: (batch_size, vocab_size)\n",
    "    loss = loss_fn(outputs, y_data_tensor)    # compare predicted token vs actual next token\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 20 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training done\")\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Test input\n",
    "test_prompt = \"The cat sat on the\"\n",
    "test_ids = tokenize(test_prompt)\n",
    "\n",
    "# Pad to match training shape\n",
    "max_len = X_data_padded.shape[1]\n",
    "if len(test_ids) < max_len:\n",
    "    test_ids += [pad_idx] * (max_len - len(test_ids))\n",
    "\n",
    "input_tensor = torch.tensor([test_ids])  # shape (1, seq_len)\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tensor)\n",
    "    predicted_token_id = torch.argmax(logits, dim=-1).item()\n",
    "    predicted_word = idx2word[predicted_token_id]\n",
    "\n",
    "print(f\"Input: {test_prompt}\")\n",
    "print(f\"Predicted next word: {predicted_word} \\n\")\n",
    "\n",
    "\n",
    "test_sentences = [\n",
    "    \"The dog slept on the\",   # â†’ bed\n",
    "    \"The bird flew over the\", # â†’ tree\n",
    "    \"The fish swam in the\",   # â†’ pond\n",
    "    \"The cow stood near the\"  # â†’ barn\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for prompt in test_sentences:\n",
    "    test_ids = tokenize(prompt)\n",
    "    if len(test_ids) < max_len:\n",
    "        test_ids += [pad_idx] * (max_len - len(test_ids))\n",
    "    \n",
    "    input_tensor = torch.tensor([test_ids])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)\n",
    "        predicted_token_id = torch.argmax(logits, dim=-1).item()\n",
    "        predicted_word = idx2word[predicted_token_id]\n",
    "\n",
    "    print(f\"Input: {prompt}\")\n",
    "    print(f\"Predicted next word: {predicted_word}\\n\")\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"tinygpt_model.pth\")\n",
    "\n",
    "# Save word2idx and idx2word for tokenizer\n",
    "with open(\"tinygpt_vocab.pkl\", \"wb\") as f:\n",
    "    pickle.dump((word2idx, idx2word), f)\n",
    "\n",
    "print(\"âœ… Model and tokenizer saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26db176b-28c9-46a2-844c-739a7edcc645",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TF 3.10)",
   "language": "python",
   "name": "tf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
